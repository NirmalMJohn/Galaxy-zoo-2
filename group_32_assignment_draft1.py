# -*- coding: utf-8 -*-
"""Group_32_Assignment_draft1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BH6XeOoNHPaBTYQV1JkfrIbH-GFLf48y
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from tensorflow.keras import layers, models, optimizers, losses, callbacks,\
                             regularizers

!unzip -q '/content/drive/MyDrive/Group_Project_Data 2.zip'

from google.colab import drive
drive.mount('/content/drive')

drive.mount('/content/drive')

batch_size = 32
img_height = 64
img_width = 64

train = tf.keras.utils.image_dataset_from_directory('/content/Group_Project_Data/Train', labels='inferred',image_size=(img_height,img_width),batch_size=batch_size,color_mode='grayscale')
valid = tf.keras.utils.image_dataset_from_directory('/content/Group_Project_Data/Valid', labels='inferred',image_size=(img_height,img_width),batch_size=batch_size,color_mode='grayscale')

class_names = train.class_names
print(class_names)

plt.figure(figsize=(10, 10))
for images, labels in train.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")



train = train.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(train))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))

valid= valid.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(valid))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))

Input= layers.Input((64,64,1), name='Input')
Conv_1 = layers.Conv2D(16, (3,3), activation='relu',
                       padding='same', name='conv_1',
                       kernel_regularizer=regularizers.l2(0.001))(Input)                     
Conv_2 = layers.Conv2D(16, (3,3), activation='relu',
                       padding='same', name='conv_2',
                       kernel_regularizer=regularizers.l2(0.001))(Conv_1)
Pool1 = layers.MaxPool2D(pool_size=(2,2),
                         name='pool_1')(Conv_2)
Conv_3 = layers.Conv2D(32, (3,3), activation='relu',
                       padding='same', name='conv_3',
                       kernel_regularizer=regularizers.l2(0.001))(Pool1)
Conv_4 = layers.Conv2D(32, (3,3), activation='relu',
                       padding='same', name='conv_4',
                       kernel_regularizer=regularizers.l2(0.001))(Conv_3)
Pool2 = layers.MaxPool2D(pool_size=(2,2),
                         name='pool_2')(Conv_4)
Conv_5 = layers.Conv2D(64, (3,3), activation='relu',
                       padding='same', name='conv_5',
                       kernel_regularizer=regularizers.l2(0.001))(Pool2)
Conv_6 = layers.Conv2D(64, (3,3), activation='relu',
                       padding='same', name='conv_6',
                       kernel_regularizer=regularizers.l2(0.001))(Conv_5)
Pool3 = layers.MaxPool2D(pool_size=(2,2),
                         name='pool_3')(Conv_6)
Flat = layers.Flatten()(Pool3)
Dense1 = layers.Dense(512, activation='relu',
                      kernel_regularizer=regularizers.l2(0.001),
                      name='fc_1')(Flat)
Dense1= layers.Dropout(0.5)(Dense1)
Dense2 = layers.Dense(1, activation='sigmoid', name='fc_2')(Dense1)
model = models.Model(inputs=Input, outputs= Dense2)
model._name = "model"
model.summary()

### Create the model using the provided architecture
Input = layers.Input((64,64,1), name='Input')
Conv_1 = layers.Conv2D(16, (3,3), activation='relu',
                      padding='same', name='conv_1')(Input)                       
Conv_2 = layers.Conv2D(16, (3,3), activation='relu',
                      padding='same', name='conv_2')(Conv_1)
Pool1 = layers.MaxPool2D(pool_size=(2,2),
                         name='pool_1')(Conv_2)
Conv_3 = layers.Conv2D(32, (3,3), activation='relu',
                      padding='same', name='conv_3')(Pool1)
Conv_4 = layers.Conv2D(32, (3,3), activation='relu',
                      padding='same', name='conv_4')(Conv_3)
Pool2 = layers.MaxPool2D(pool_size=(2,2),
                         name='pool_2')(Conv_4)
Conv_5 = layers.Conv2D(64, (3,3), activation='relu',
                      padding='same', name='conv_5')(Pool2)
Conv_6 = layers.Conv2D(64, (3,3), activation='relu',
                      padding='same', name='conv_6')(Conv_5)
Pool3 = layers.MaxPool2D(pool_size=(2,2),
                         name='pool_3')(Conv_6)
Flat = layers.Flatten()(Pool3)
Dense1 = layers.Dense(512, activation='relu', name='fc_1')(Flat)
Dense1= layers.Dropout(0.5)(Dense1)
Dense2 = layers.Dense(1, activation='sigmoid', name='fc_2')(Dense1)
model = models.Model(inputs=Input, outputs= Dense2)
model._name = "model"
model.summary()

def reinitialize(model):
    # Loop over the layers of the model
    for l in model.layers:
        # Check if the layer has initializers
        if hasattr(l,"kernel_initializer"):
            # Reset the kernel weights
            l.kernel.assign(l.kernel_initializer(tf.shape(l.kernel)))
        if hasattr(l,"bias_initializer"):
            # Reset the bias
            l.bias.assign(l.bias_initializer(tf.shape(l.bias)))

import pandas as pd
optimizers = ['Adam','SGD','RMSprop']
lr=[0.001]
hist=pd.DataFrame()
for i in optimizers:
  for k in lr:
    reinitialize(model)
    print(f' For Optimizer:{i}, Learning Rate{k}')
    model.compile(optimizer=i,loss='binary_crossentropy',
                        metrics=['accuracy'])
    model.optimizer.learning_rate = k
    history = model.fit(train, validation_data=(valid),
                              epochs=25, batch_size=32)
    hist_df = pd.DataFrame(history.history) 
    hist_df['Optimizer']= i
    hist_df['learning rate']=k
    hist= pd.concat([hist, hist_df], axis=0)

### Create plots that show the losses and metrics for each of these runs, and
### comment on the results in your report.

#ad_df1 =hist.groupby(['Optimizer','learning rate']).get_group(('Adam',0.1))
#ad_df2 =hist.groupby(['Optimizer','learning rate']).get_group(('Adam',0.01))
ad_df3 =hist.groupby(['Optimizer','learning rate']).get_group(('Adam',0.001))
#sg_df1=hist.groupby(['Optimizer','learning rate']).get_group(('SGD',0.1))
#sg_df2=hist.groupby(['Optimizer','learning rate']).get_group(('SGD',0.01))
sg_df3=hist.groupby(['Optimizer','learning rate']).get_group(('SGD',0.001))
#rm_df1=hist.groupby(['Optimizer','learning rate']).get_group(('RMSprop',0.1))
#rm_df2=hist.groupby(['Optimizer','learning rate']).get_group(('RMSprop',0.01))
rm_df3=hist.groupby(['Optimizer','learning rate']).get_group(('RMSprop',0.001))

# Plotting the Losses of  different  optimizers for different learning rate
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,15),
                         constrained_layout=True)
#ad_df1.plot(y=['loss','val_loss'],ax=axes[0,0],title='Loss(Adam,LR=0.1)',
#            xlabel='No of epochs',)
#ad_df2.plot(y=['loss','val_loss'],ax=axes[0,1],title='Loss(Adam,LR=0.01)',
#           xlabel='No of epochs',)
ad_df3.plot(y=['loss','val_loss'],ax=axes[0],title='Loss(Adam,LR=0.001)',
            xlabel='No of epochs')
#sg_df1.plot(y=['loss','val_loss'],ax=axes[1,0],title='Loss(SGD,LR=0.1)',
#            xlabel='No of epochs',)
#sg_df2.plot(y=['loss','val_loss'],ax=axes[1,1],title='Loss(SGD,LR=0.01)',
#            xlabel='No of epochs',)
sg_df3.plot(y=['loss','val_loss'],ax=axes[1],title='Loss(SGD,LR=0.001)',
            xlabel='No of epochs',)
#rm_df1.plot(y=['loss','val_loss'],ax=axes[2,0],title='Loss(RMSprop,LR=0.1)',
#           xlabel='No of epochs',)
#rm_df1.plot(y=['loss','val_loss'],ax=axes[2,1],title='Loss(RMSprop,LR=0.01)',
#            xlabel='No of epochs',)
rm_df3.plot(y=['loss','val_loss'],ax=axes[2],title='Loss(RMSprop,LR=0.001)',
            xlabel='No of epochs',)

# Plotting the Accuracy of  different  optimizers for different learning rate
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,15),
                         constrained_layout=True)
#ad_df1.plot(y=['accuracy','val_accuracy'],ax=axes[0,0], xlabel='No of epochs',
#            title='Accuracy & Validation Accuracy (Adam,LR=0.1)')
#ad_df2.plot(y=['accuracy','val_accuracy'],ax=axes[0,1],xlabel='No of epochs',
#            title='Accuracy & Vaidation Accuracy (Adam,LR=0.01)')
ad_df3.plot(y=['accuracy','val_accuracy'],ax=axes[0],xlabel='No of epochs',
            title='Accuracy & Validation Accuracy (Adam,LR=0.001)')
#sg_df1.plot(y=['accuracy','val_accuracy'],ax=axes[1,0],xlabel='No of epochs',
#            title='Accuracy & Validation Accuracy (SGD,LR=0.1)')
#sg_df2.plot(y=['accuracy','val_accuracy'],ax=axes[1,1],xlabel='No of epochs',
#            title='Accyracy & Validation Accuracy (SGD,LR=0.01)')
sg_df3.plot(y=['accuracy','val_accuracy'],ax=axes[1],xlabel='No of epochs',
            title='Accuracy & Validation Accuracy(SGD,LR=0.001)')
#rm_df1.plot(y=['accuracy','val_accuracy'],ax=axes[2,0],xlabel='No of epochs',
#            title='Accuracy & Validation Accuracy (RMSprop,LR=0.1)')
#rm_df1.plot(y=['accuracy','val_accuracy'],ax=axes[2,1],xlabel='No of epochs',
#            title='(RMSprop,LR=0.01)')
rm_df3.plot(y=['accuracy','val_accuracy'],ax=axes[2],xlabel='No of epochs',
            title='Accuracy & Validation Accuracy (RMSprop,LR=0.001)')

